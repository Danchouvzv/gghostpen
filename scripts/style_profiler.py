#!/usr/bin/env python3
"""
Style Profiler –¥–ª—è GhostPen.

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Å—Ç—ã –∞–≤—Ç–æ—Ä–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Å–æ–∑–¥–∞—ë—Ç —Å—Ç–∏–ª–µ–≤—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏,
–∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤ –≤ –∞–≤—Ç–æ—Ä—Å–∫–æ–º —Å—Ç–∏–ª–µ.
"""

import json
import re
import statistics
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import Counter
from datetime import datetime, timezone


class StyleProfiler:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç–∏–ª—è –∞–≤—Ç–æ—Ä–∞."""
    
    # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞
    FORMAL_WORDS = ['–ø–æ–Ω–∏–º–∞—é', '–ø—Ä–∏–º–µ–Ω—è—é', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '—Å–ª–µ–¥—É–µ—Ç', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', 
                    '–≤–∞–∂–Ω–æ', '–∫–ª—é—á–µ–≤–æ–π', '–ø—Ä–∏–Ω—Ü–∏–ø', '–ø–æ–¥—Ö–æ–¥', '–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è']
    EMOTIONAL_WORDS = ['—á—É–≤—Å—Ç–≤—É—é', '–ª—é–±–ª—é', '–Ω—Ä–∞–≤–∏—Ç—Å—è', '–≤–æ–ª–Ω—É—é—Å—å', '—Å—Ç—Ä–∞—à–Ω–æ', 
                       '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ', '–≤–¥–æ—Ö–Ω–æ–≤–ª—è–µ—Ç', '—Ä–∞–¥—É–µ—Ç']
    EXPERT_WORDS = ['–∞–Ω–∞–ª–∏–∑', '—Ä–µ—à–µ–Ω–∏–µ', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', '–º–µ—Ç—Ä–∏–∫–∏',
                    '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ø—Ä–æ—Ü–µ—Å—Å', '—Å–∏—Å—Ç–µ–º–∞']
    CASUAL_WORDS = ['–∫—Å—Ç–∞—Ç–∏', '–≤–æ–æ–±—â–µ', '–∫–æ—Ä–æ—á–µ', '—Ç–∏–ø–∞', '–∫–∞–∫ –±—ã', '–≤ –æ–±—â–µ–º']
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ signature_phrases
    STOP_PHRASES = {
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        '–≤—á–µ—Ä–∞ –Ω–∞', '—Å–µ–≥–æ–¥–Ω—è —è', '—Å–µ–≥–æ–¥–Ω—è —É—Ç—Ä–æ–º', '—Å–µ–≥–æ–¥–Ω—è –≤–µ—á–µ—Ä–æ–º', '–Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ',
        '–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ', '–Ω–µ–¥–∞–≤–Ω–æ —è', '–Ω–µ–¥–∞–≤–Ω–æ –º—ã', '–≤—á–µ—Ä–∞ –ø–æ–ª—É—á–∏–ª', '—Å–µ–≥–æ–¥–Ω—è –Ω–∞—á–∞–ª',
        # –û–±—â–∏–µ —Ñ—Ä–∞–∑—ã
        '—ç—Ç–æ –Ω–µ', '—ç—Ç–æ –ø—Ä–æ', '—á—Ç–æ –≤—ã', '–∫–∞–∫ –≤—ã', '—ç—Ç–æ –∑–Ω–∞—á–∏—Ç', '—ç—Ç–æ —Ç–æ', '—ç—Ç–æ –∫–∞–∫',
        '–∫–æ–≥–¥–∞ –º—ã', '–∫–æ–≥–¥–∞ —Ç—ã', '–∫–æ–≥–¥–∞ —è', '–µ—Å–ª–∏ –≤—ã', '–µ—Å–ª–∏ –º—ã', '–µ—Å–ª–∏ —Ç—ã',
        '–¥–ª—è —Ç–æ–≥–æ', '–¥–ª—è –≤–∞—Å', '–¥–ª—è –Ω–∞—Å', '–¥–ª—è –º–µ–Ω—è', '–¥–ª—è —Ç–µ–±—è',
        '–º–æ–∂–µ—Ç –±—ã—Ç—å', '–º–æ–∂–µ—Ç –±—ã—Ç—å', '–º–æ–∂–µ—Ç –±—ã—Ç—å', '–º–æ–∂–µ—Ç –±—ã—Ç—å',
        '–≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å', '–≤—Å–µ–≥–¥–∞ –º–æ–∂–Ω–æ', '–≤—Å–µ–≥–¥–∞ –Ω—É–∂–Ω–æ', '–≤—Å–µ–≥–¥–∞ –≤–∞–∂–Ω–æ',
        '–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ', '–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ', '–æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ',
        '–Ω–µ —Ç–æ–ª—å–∫–æ', '–Ω–µ –ø—Ä–æ—Å—Ç–æ', '–Ω–µ –≤—Å–µ–≥–¥–∞', '–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ',
        '—Ç–æ –µ—Å—Ç—å', '—Ç–æ –µ—Å—Ç—å', '—Ç–æ –µ—Å—Ç—å', '—Ç–æ –µ—Å—Ç—å',
        '—Ç–∞–∫ —á—Ç–æ', '—Ç–∞–∫ —á—Ç–æ', '—Ç–∞–∫ —á—Ç–æ', '—Ç–∞–∫ —á—Ç–æ',
        '–∏ —ç—Ç–æ', '–∏ —ç—Ç–æ', '–∏ —ç—Ç–æ', '–∏ —ç—Ç–æ',
        '–Ω–æ —ç—Ç–æ', '–Ω–æ —ç—Ç–æ', '–Ω–æ —ç—Ç–æ', '–Ω–æ —ç—Ç–æ',
        '–∏–ª–∏ —ç—Ç–æ', '–∏–ª–∏ —ç—Ç–æ', '–∏–ª–∏ —ç—Ç–æ', '–∏–ª–∏ —ç—Ç–æ',
        # –í–æ–ø—Ä–æ—Å—ã
        '—á—Ç–æ –¥—É–º–∞–µ—Ç–µ', '—á—Ç–æ –≤—ã –¥—É–º–∞–µ—Ç–µ', '–∫–∞–∫ –≤—ã –¥—É–º–∞–µ—Ç–µ', '—á—Ç–æ –≤—ã',
        '–∫–∞–∫ –≤—ã', '—á—Ç–æ –¥—É–º–∞–µ—à—å', '–∫–∞–∫ –¥—É–º–∞–µ—à—å',
        # –°–≤—è–∑–∫–∏
        '–∞ —Ç–∞–∫–∂–µ', '–∞ —Ç–∞–∫–∂–µ', '–∞ —Ç–∞–∫–∂–µ', '–∞ —Ç–∞–∫–∂–µ',
        '–∏ –µ—â—ë', '–∏ –µ—â—ë', '–∏ –µ—â—ë', '–∏ –µ—â—ë',
        '–Ω–æ –∏', '–Ω–æ –∏', '–Ω–æ –∏', '–Ω–æ –∏',
    }
    
    # –¢–µ–º—ã
    TOPICS = {
        'career': ['–∫–∞—Ä—å–µ—Ä–∞', '—Ä–∞–±–æ—Ç–∞', '–ø—Ä–æ—Ñ–µ—Å—Å–∏—è', '–∫–æ–º–∞–Ω–¥–∞', '–ø—Ä–æ–µ–∫—Ç', '–ª–∏–¥–µ—Ä—Å—Ç–≤–æ', '–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç'],
        'motivation': ['–º–æ—Ç–∏–≤–∞—Ü–∏—è', '—Ü–µ–ª—å', '—Ä–æ—Å—Ç', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '—É—Å–ø–µ—Ö', '–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ', '–≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏–µ'],
        'personal': ['–ª–∏—á–Ω—ã–π', '–æ–ø—ã—Ç', '–∏—Å—Ç–æ—Ä–∏—è', '—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ', '–º—ã—Å–ª—å', '—á—É–≤—Å—Ç–≤–æ'],
        'expertise': ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç', '–º–µ—Ç–æ–¥', '–ø–æ–¥—Ö–æ–¥', '—Ä–µ—à–µ–Ω–∏–µ', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è'],
        'business': ['–±–∏–∑–Ω–µ—Å', '—Å—Ç–∞—Ä—Ç–∞–ø', '–∫–ª–∏–µ–Ω—Ç', '–ø—Ä–æ–¥—É–∫—Ç', '—Ä—ã–Ω–æ–∫', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è']
    }
    
    def __init__(self):
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE
        )
    
    def analyze_author(self, author_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∏–ª—å –∞–≤—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –µ–≥–æ –ø–æ—Å—Ç–æ–≤.
        
        Args:
            author_data: –î–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ—Ä–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
            
        Returns:
            –°—Ç–∏–ª–µ–≤–æ–π –ø—Ä–æ—Ñ–∏–ª—å –∞–≤—Ç–æ—Ä–∞
        """
        author_id = author_data["author_id"]
        all_posts = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã –∞–≤—Ç–æ—Ä–∞
        for platform, posts in author_data.get("platforms", {}).items():
            for post in posts:
                all_posts.append({
                    "content": post["content"],
                    "platform": platform,
                    "meta": post.get("meta", {})
                })
        
        if not all_posts:
            return self._empty_profile(author_id)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª—å
        profile = {
            "author_id": author_id,
            "generated_at": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
            "total_posts": len(all_posts),
            "platforms": list(author_data.get("platforms", {}).keys()),
            "style": self._analyze_style(all_posts),
            "platform_specific": self._analyze_platforms(all_posts),
            "topics": self._detect_topics(all_posts),
            "signature_phrases": self._extract_phrases(all_posts),
            "sample_posts": self._get_sample_posts(all_posts, max_samples=3)
        }
        
        return profile
    
    def _analyze_style(self, posts: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—â–∏–π —Å—Ç–∏–ª—å –∞–≤—Ç–æ—Ä–∞."""
        all_text = " ".join([p["content"] for p in posts])
        sentences = self._split_sentences(all_text)
        
        # –î–ª–∏–Ω–∞
        post_lengths = [len(p["content"]) for p in posts]
        sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞
        paragraphs_per_post = [p["content"].count("\n\n") + 1 for p in posts]
        has_lists = sum(1 for p in posts if re.search(r'^\d+\.|^[-‚Ä¢]', p["content"], re.MULTILINE))
        
        # –≠–º–æ–¥–∑–∏ –∏ —Ö—ç—à—Ç–µ–≥–∏
        total_emojis = sum(len(p.get("meta", {}).get("emojis", [])) for p in posts)
        total_hashtags = sum(len(p.get("meta", {}).get("hashtags", [])) for p in posts)
        emoji_density = total_emojis / len(posts) if posts else 0
        hashtag_density = total_hashtags / len(posts) if posts else 0
        
        # –¢–æ–Ω
        tone_scores = self._analyze_tone(all_text)
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        emotionality = self._calculate_emotionality(all_text)
        
        return {
            "avg_post_length": int(statistics.mean(post_lengths)) if post_lengths else 0,
            "min_post_length": min(post_lengths) if post_lengths else 0,
            "max_post_length": max(post_lengths) if post_lengths else 0,
            "avg_sentence_length": float(statistics.mean(sentence_lengths)) if sentence_lengths else 0,
            "avg_paragraphs_per_post": float(statistics.mean(paragraphs_per_post)) if paragraphs_per_post else 0,
            "uses_lists": has_lists > 0,
            "list_frequency": has_lists / len(posts) if posts else 0,
            "emoji_density": round(emoji_density, 2),
            "hashtag_density": round(hashtag_density, 2),
            "tone": tone_scores,
            "emotionality": round(emotionality, 2),
            "structure_type": self._detect_structure_type(posts)
        }
    
    def _analyze_platforms(self, posts: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∏–ª—å –ø–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º."""
        platform_data = {}
        
        for platform in ['linkedin', 'instagram', 'facebook', 'telegram']:
            platform_posts = [p for p in posts if p["platform"] == platform]
            if not platform_posts:
                continue
            
            platform_text = " ".join([p["content"] for p in platform_posts])
            sentences = self._split_sentences(platform_text)
            
            post_lengths = [len(p["content"]) for p in platform_posts]
            sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
            
            total_emojis = sum(len(p.get("meta", {}).get("emojis", [])) for p in platform_posts)
            total_hashtags = sum(len(p.get("meta", {}).get("hashtags", [])) for p in platform_posts)
            
            platform_data[platform] = {
                "post_count": len(platform_posts),
                "avg_length": int(statistics.mean(post_lengths)) if post_lengths else 0,
                "avg_sentence_length": float(statistics.mean(sentence_lengths)) if sentence_lengths else 0,
                "emoji_density": round(total_emojis / len(platform_posts), 2) if platform_posts else 0,
                "hashtag_density": round(total_hashtags / len(platform_posts), 2) if platform_posts else 0,
                "tone": self._analyze_tone(platform_text)
            }
        
        return platform_data
    
    def _analyze_tone(self, text: str) -> Dict[str, float]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ–Ω —Ç–µ–∫—Å—Ç–∞."""
        text_lower = text.lower()
        
        formal_score = sum(1 for word in self.FORMAL_WORDS if word in text_lower) / max(len(text.split()), 1) * 1000
        emotional_score = sum(1 for word in self.EMOTIONAL_WORDS if word in text_lower) / max(len(text.split()), 1) * 1000
        expert_score = sum(1 for word in self.EXPERT_WORDS if word in text_lower) / max(len(text.split()), 1) * 1000
        casual_score = sum(1 for word in self.CASUAL_WORDS if word in text_lower) / max(len(text.split()), 1) * 1000
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π —Ç–æ–Ω
        scores = {
            "formal": round(formal_score, 2),
            "emotional": round(emotional_score, 2),
            "expert": round(expert_score, 2),
            "casual": round(casual_score, 2)
        }
        
        dominant = max(scores.items(), key=lambda x: x[1])[0]
        scores["dominant"] = dominant
        
        return scores
    
    def _calculate_emotionality(self, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏."""
        emotional_words = sum(1 for word in self.EMOTIONAL_WORDS if word in text.lower())
        emojis_count = len(self.emoji_pattern.findall(text))
        exclamation_count = text.count('!')
        question_count = text.count('?')
        
        total_words = len(text.split())
        if total_words == 0:
            return 0.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        emotionality = (emotional_words * 2 + emojis_count * 3 + exclamation_count + question_count) / total_words * 100
        
        return min(emotionality, 10.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º
    
    def _detect_structure_type(self, posts: List[Dict]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ—Å—Ç–æ–≤."""
        has_numbered_lists = sum(1 for p in posts if re.search(r'^\d+\.', p["content"], re.MULTILINE))
        has_bullet_lists = sum(1 for p in posts if re.search(r'^[-‚Ä¢]', p["content"], re.MULTILINE))
        has_paragraphs = sum(1 for p in posts if '\n\n' in p["content"])
        
        if has_numbered_lists > len(posts) * 0.3:
            return "numbered_lists"
        elif has_bullet_lists > len(posts) * 0.3:
            return "bullet_lists"
        elif has_paragraphs > len(posts) * 0.5:
            return "paragraphs"
        else:
            return "narrative"
    
    def _detect_topics(self, posts: List[Dict]) -> Dict[str, float]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º–∞—Ç–∏–∫—É –ø–æ—Å—Ç–æ–≤."""
        all_text = " ".join([p["content"].lower() for p in posts])
        topic_scores = {}
        
        for topic, keywords in self.TOPICS.items():
            matches = sum(1 for keyword in keywords if keyword in all_text)
            topic_scores[topic] = round(matches / len(self.TOPICS[topic]) / max(len(posts), 1) * 100, 2)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        sorted_topics = dict(sorted(topic_scores.items(), key=lambda x: x[1], reverse=True))
        
        return sorted_topics
    
    def _extract_phrases(self, posts: List[Dict], max_phrases: int = 5) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∞–≤—Ç–æ—Ä–∞."""
        # –ò—â–µ–º —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã –∏–∑ 2-3 —Å–ª–æ–≤
        all_text = " ".join([p["content"] for p in posts])
        words = re.findall(r'\b\w+\b', all_text.lower())
        
        # –ë–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã
        bigrams = [f"{words[i]} {words[i+1]}" for i in range(len(words)-1)]
        trigrams = [f"{words[i]} {words[i+1]} {words[i+2]}" for i in range(len(words)-2)]
        
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        phrase_counter = Counter(bigrams + trigrams)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã –∏ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã
        filtered_phrases = []
        for phrase, count in phrase_counter.most_common(max_phrases * 3):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã
            if phrase.lower() in self.STOP_PHRASES:
                continue
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ (–º–µ–Ω—å—à–µ 4 —Å–∏–º–≤–æ–ª–æ–≤)
            if len(phrase) < 4:
                continue
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ—Ä–∞–∑—ã —Ç–æ–ª—å–∫–æ –∏–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
            words = phrase.split()
            if len(words) < 2:
                continue
            # –ú–∏–Ω–∏–º—É–º 2 –≤—Ö–æ–∂–¥–µ–Ω–∏—è
            if count < 2:
                continue
            filtered_phrases.append(phrase)
            if len(filtered_phrases) >= max_phrases:
                break
        
        return filtered_phrases
    
    def _get_sample_posts(self, posts: List[Dict], max_samples: int = 3) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –ø–æ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞."""
        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Å—Ç—ã —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω—ã (–Ω–µ —Å–∞–º—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –Ω–µ —Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ)
        sorted_posts = sorted(posts, key=lambda p: len(p["content"]))
        start_idx = len(sorted_posts) // 4
        end_idx = start_idx + max_samples
        
        samples = sorted_posts[start_idx:end_idx]
        return [p["content"][:500] + "..." if len(p["content"]) > 500 else p["content"] 
                for p in samples]
    
    def _split_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        sentences = re.split(r'[.!?]+\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _empty_profile(self, author_id: str) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π –ø—Ä–æ—Ñ–∏–ª—å –¥–ª—è –∞–≤—Ç–æ—Ä–∞ –±–µ–∑ –ø–æ—Å—Ç–æ–≤."""
        return {
            "author_id": author_id,
            "generated_at": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
            "total_posts": 0,
            "platforms": [],
            "style": {},
            "platform_specific": {},
            "topics": {},
            "signature_phrases": [],
            "sample_posts": []
        }


def generate_profiles(dataset_path: Path, output_path: Path) -> None:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∏–ª–µ–≤—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è –≤—Å–µ—Ö –∞–≤—Ç–æ—Ä–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞.
    
    Args:
        dataset_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞—Ç–∞—Å–µ—Ç–∞
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π
    """
    print(f"üìñ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ {dataset_path}...")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    profiler = StyleProfiler()
    profiles = []
    
    print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(dataset['authors'])} –∞–≤—Ç–æ—Ä–æ–≤...")
    for author in dataset['authors']:
        author_id = author['author_id']
        print(f"  ‚Üí {author_id}...", end=' ', flush=True)
        
        profile = profiler.analyze_author(author)
        profiles.append(profile)
        
        print(f"‚úì ({profile['total_posts']} –ø–æ—Å—Ç–æ–≤)")
    
    result = {
        "version": "1.0",
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "profiles": profiles
    }
    
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –ø—Ä–æ—Ñ–∏–ª–∏ –≤ {output_path}...")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω–æ {len(profiles)} –ø—Ä–æ—Ñ–∏–ª–µ–π.")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import sys
    
    if len(sys.argv) < 3:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python style_profiler.py <dataset.json> <output_profiles.json>")
        sys.exit(1)
    
    dataset_path = Path(sys.argv[1])
    output_path = Path(sys.argv[2])
    
    if not dataset_path.exists():
        print(f"‚ùå –§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")
        sys.exit(1)
    
    generate_profiles(dataset_path, output_path)


if __name__ == "__main__":
    main()

